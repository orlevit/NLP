{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e3ca4e9-90a2-49ae-b1a8-98e83a6438e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://jalammar.github.io/illustrated-gpt2/#part-3-beyond-language-modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "174393ba-b48f-4156-97d8-e83ba4dd206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "checkpoint = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = GPT2LMHeadModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "2a2635f7-8694-4534-a24a-45a5460f9868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_vocab_stat(tokenizer, voc_size_sample=20):\n",
    "    initial_vocab = list(tokenizer.vocab.keys())\n",
    "    \n",
    "    print(f\"Vocabulary length:\\033[1m{len(initial_vocab)}\\033[0m\\n\")\n",
    "    print(f\"Special tokens: {tokenizer.all_special_tokens}\\n\")\n",
    "    print(f\"Loaded vocabulary tokens, samples:\\n{initial_vocab[:voc_size_sample]}\")\n",
    "    \n",
    "\n",
    "def viz_sentence_tokens(raw_inputs, examples=-1):\n",
    "    inputs = tokenizer(raw_inputs, truncation=True, return_overflowing_tokens=True,return_length=True)#, return_tensors=\"pt\")\n",
    "    examples = len(raw_inputs) if examples == -1 else examples\n",
    "    for i, raw, processed in zip(range(examples), raw_inputs, inputs[\"input_ids\"]):\n",
    "        print(\"\\n\")\n",
    "        print(f\"Example:{i}\")\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        print(raw)\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        print(tokenizer.tokenize(raw))\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        print(tokenizer.decode(processed))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    for sentense in raw_inputs:\n",
    "        tokens = tokenizer.tokenize(sentense)\n",
    "        if tokenizer.unk_token in tokens:\n",
    "            print(\"Simple [UNK] check:\")\n",
    "            print(sentense)\n",
    "            print(tokens)\n",
    "            \n",
    "def viz_not_existing_tokens(raw_inputs, examples=-1):\n",
    "    initial_vocab = list(tokenizer.vocab.keys())\n",
    "    examples = len(raw_inputs) if examples == -1 else examples\n",
    "    inputs = tokenizer(raw_inputs, truncation=True, return_overflowing_tokens=True,return_length=True)\n",
    "    for i, sentense in zip(range(examples), raw_inputs):\n",
    "        print(f\"+++++++++++++++++++++++++++++ sentence number {i} +++++++++++++++++++++++++++++\")\n",
    "        print(f\"The sentence: {sentense}\")\n",
    "        not_existing = []\n",
    "        for word in sentense.split():\n",
    "            if word not in initial_vocab: \n",
    "                not_existing.append(word)\n",
    "        print(f\"Not existing words: {not_existing}\")\n",
    "        print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        print(\"\\n\")\n",
    "                \n",
    "def add_tokens(additional_tokens, model):\n",
    "    tokenizer.add_tokens(additional_tokens)\n",
    "    # Additionl rows are added at the end of the embeddings\n",
    "    # The initialization is like in \"torch.nn.Embeddin\" - normal(0,1)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    return tokenizer, model\n",
    "\n",
    "def print_model_arch(model):\n",
    "    print([module for module in model.modules()])\n",
    "    \n",
    "def print_model_total_params(model):\n",
    "    total_parmeters = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Parameters number(Including embeddings): {total_parmeters /1000**2}M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "1f9356dc-299b-44c8-990a-e48ba2fc3b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base on samples from https://www.kaggle.com/jeet2016/us-financial-news-articles\n",
    "raw_inputs_train = [\"UK's Compass says new CRCC Co. CEO to start Jan 1 after death of Cousins - Reuters\",\n",
    "                    \"scenes from deadly protests in Iran STR | AFP | Getty Images 5 Mins Ago Unrest in Iran\"]\n",
    "raw_inputs_valid = [\"Tesla delivers 1,550 Model 3 sedans and 29,870 total vehicles in fourth quarter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c187999-2996-4bc1-ba46-b425b2946b57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_vocab_stat(tokenizer, voc_size_sample=30)\n",
    "viz_sentence_tokens(raw_inputs, examples=-1)\n",
    "#viz_not_existing_tokens(raw_inputs, examples=-1)\n",
    "#print_model_arch(model)\n",
    "#print_model_total_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "c76e05c9-d436-4101-bbb9-69fb849eaa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_tokens = [\"CRCC\", \"seden\"]\n",
    "tokenizer, model = add_tokens(additional_tokens, model)\n",
    "df1 = pd.DataFrame(raw_inputs_train, columns=[\"content\"])\n",
    "df2 = pd.DataFrame(raw_inputs_valid, columns=[\"content\"])\n",
    "raw_dataset_train = Dataset.from_pandas(df1)\n",
    "raw_dataset_valid = Dataset.from_pandas(df1)\n",
    "raw_datasets = DatasetDict({\"train\": raw_dataset_train, \"valid\": raw_dataset_valid})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "4735bb72-6e22-48dd-8c01-e03ca790a99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 154.89ba/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 273.37ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(element):\n",
    "    context_length = 9\n",
    "    outputs = tokenizer(\n",
    "        element[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "# Noteice that the addition of the new words are ignored! when retokenized the dataset by the modle tokenized!!!\n",
    "tokenized_datasets = raw_datasets.map(tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "bf6aef05-0983-41f0-ab74-4517a1eeb26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"codeparrot-ds\",\n",
    "    per_device_train_batch_size=1,#32,\n",
    "    per_device_eval_batch_size=1,#32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1,#5_000,\n",
    "    logging_steps=1,#,5_000,\n",
    "    gradient_accumulation_steps=1,#8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=2#5_000,\n",
    "    #fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  tokenizer=tokenizer,\n",
    "                  args=args,\n",
    "                  data_collator=data_collator,\n",
    "                  train_dataset=tokenized_datasets[\"train\"],\n",
    "                  eval_dataset=tokenized_datasets[\"valid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6380f79-d549-460a-8a45-7fa3b42b121b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/or/Downloads/learned_models/com/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/8 : < :, Epoch 0.25/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.944100</td>\n",
       "      <td>6.675638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 1\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f949da-3bba-423e-ace4-f890f577da55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
